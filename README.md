# A very personal selection of robotics (and related) papers
*Find the value of the papers **on your own** and don't be biased by anyone and any media, including me.*

Paper are organized by topics and years. More to be added. 

## Control by Code Generation
### 2022
- Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language [arxiv](https://arxiv.org/abs/2204.00598)
- Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [paper](https://say-can.github.io/assets/palm_saycan.pdf)
### 2023
- Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents [paper](https://grounded-decoding.github.io/paper.pdf)
- VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models [paper](https://voxposer.github.io/voxposer.pdf)


## Reward Function by Code Generation
### 2022
- Code as Policies: Language Model Programs for Embodied Control [arxiv](https://arxiv.org/abs/2209.07753)
### 2023
- Language to Rewards for Robotic Skill Synthesis [arxiv](https://arxiv.org/pdf/2306.08647.pdf)
- Eureka: Human-Level Reward Design via Coding Large Language Models [arxiv](https://arxiv.org/pdf/2310.12931.pdf)


## LLM as Reward Functions
### 2022
- Can foundation models perform zero-shot task specification for robot manipulation? [paper](https://arxiv.org/abs/2204.11134)
- Zero-Shot Reward Specification via Grounded Natural Language [paper](https://proceedings.mlr.press/v162/mahmoudieh22a.html)
### 2023
- Vision-language models as success detectors [paper](https://arxiv.org/abs/2303.07280)
- RoboCLIP: One Demonstration is Enough to Learn Robot Policies [paper](https://arxiv.org/pdf/2310.07899.pdf)
- Vision-Language Models as a Source of Rewards [arxiv](https://arxiv.org/abs/2312.09187)

## End-to-End Foundation Policy Model
### 2021
- CLIPORT: What and Where Pathways for Robotic Manipulation [arxiv](https://arxiv.org/pdf/2109.12098.pdf)
### 2022
- RT-1: Robotics Transformer for Real-World Control at Scale [arxiv](https://arxiv.org/abs/2212.06817)
### 2023
- PaLM-E: An Embodied Multimodal Language Model [arxiv](https://arxiv.org/abs/2303.03378)
- RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [arxiv](https://arxiv.org/abs/2307.15818)

## Datasets
### 2023
- Open X-Embodiment: Robotic Learning Datasets and RT-X Models [website](https://robotics-transformer-x.github.io/)[arxiv](https://arxiv.org/abs/2310.08864)

## Cross-embodiment
### 2022
- XIRL: Cross-embodiment Inverse Reinforcement Learning [paper](https://proceedings.mlr.press/v164/zakka22a/zakka22a.pdf)

### 2023
- Polybot: Training One Policy Across Robots While Embracing Variability [paper](https://openreview.net/pdf?id=HEIRj51lcS)

## Bi-manual Operation
### 2020
- Deep Imitation Learning for Bimanual Robotic Manipulation [arxiv](https://arxiv.org/pdf/2010.05134.pdf)
### 2023
- Learning Fine-Grained Bimanual Manipulation with
Low-Cost Hardware [arxiv](https://arxiv.org/pdf/2304.13705.pdf)

## Autonomous Agent / All Generated
- GenSim: Generating Robotic Simulation Tasks via Large Language Models [arxiv](https://arxiv.org/abs/2310.01361)

## World Model



